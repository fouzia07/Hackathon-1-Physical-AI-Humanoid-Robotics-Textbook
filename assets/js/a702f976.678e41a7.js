"use strict";(globalThis.webpackChunkai_textbook=globalThis.webpackChunkai_textbook||[]).push([[4033],{7570(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"modules/module-2/chapter-3-reinforcement-learning-physical-ai","title":"Chapter 3 - Reinforcement Learning for Physical AI","description":"Understanding reinforcement learning and its applications in Physical AI systems","source":"@site/docs/modules/module-2/chapter-3-reinforcement-learning-physical-ai.md","sourceDirName":"modules/module-2","slug":"/modules/module-2/chapter-3-reinforcement-learning-physical-ai","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/modules/module-2/chapter-3-reinforcement-learning-physical-ai","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-2/chapter-3-reinforcement-learning-physical-ai.md","tags":[{"inline":true,"label":"Reinforcement Learning","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/tags/reinforcement-learning"},{"inline":true,"label":"AI","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/tags/ai"},{"inline":true,"label":"textbook","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/tags/textbook"},{"inline":true,"label":"education","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/tags/education"},{"inline":true,"label":"Physical AI","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/tags/physical-ai"},{"inline":true,"label":"Robotics","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/tags/robotics"}],"version":"current","frontMatter":{"title":"Chapter 3 - Reinforcement Learning for Physical AI","description":"Understanding reinforcement learning and its applications in Physical AI systems","tags":["Reinforcement Learning","AI","textbook","education","Physical AI","Robotics"],"learning_objectives":["Understand RL fundamentals and Q-learning algorithms","Learn about policy gradients in robotics","Explore deep RL applications in Physical AI"],"summary":"This chapter explores reinforcement learning concepts, algorithms, and their applications in Physical AI systems for learning complex behaviors through interaction with the environment."},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2 - Deep Learning for Physical AI","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/modules/module-2/chapter-2-deep-learning-physical-ai"},"next":{"title":"Chapter 1 - NVIDIA Isaac Sim","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/modules/module-3-nvidia-isaac/chapter-1-nvidia-isaac-sim"}}');var a=i(4848),r=i(8453);const s={title:"Chapter 3 - Reinforcement Learning for Physical AI",description:"Understanding reinforcement learning and its applications in Physical AI systems",tags:["Reinforcement Learning","AI","textbook","education","Physical AI","Robotics"],learning_objectives:["Understand RL fundamentals and Q-learning algorithms","Learn about policy gradients in robotics","Explore deep RL applications in Physical AI"],summary:"This chapter explores reinforcement learning concepts, algorithms, and their applications in Physical AI systems for learning complex behaviors through interaction with the environment."},o="Chapter 3 - Reinforcement Learning for Physical AI",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Concept",id:"concept",level:2},{value:"RL Fundamentals",id:"rl-fundamentals",level:3},{value:"Q-Learning and Deep Q-Networks",id:"q-learning-and-deep-q-networks",level:3},{value:"Policy Gradient Methods",id:"policy-gradient-methods",level:3},{value:"System",id:"system",level:2},{value:"Simple Q-Learning Example",id:"simple-q-learning-example",level:3},{value:"Practice",id:"practice",level:2},{value:"Exercise 1: Grid World Navigation",id:"exercise-1-grid-world-navigation",level:3},{value:"Exercise 2: Robot Arm Control",id:"exercise-2-robot-arm-control",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-3---reinforcement-learning-for-physical-ai",children:"Chapter 3 - Reinforcement Learning for Physical AI"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Understand RL fundamentals and Q-learning algorithms"}),"\n",(0,a.jsx)(n.li,{children:"Learn about policy gradients in robotics"}),"\n",(0,a.jsx)(n.li,{children:"Explore deep RL applications in Physical AI"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"concept",children:"Concept"}),"\n",(0,a.jsx)(n.p,{children:"Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. For Physical AI systems, RL enables robots to learn complex behaviors, navigation, manipulation, and adaptive control strategies."}),"\n",(0,a.jsx)(n.h3,{id:"rl-fundamentals",children:"RL Fundamentals"}),"\n",(0,a.jsx)(n.p,{children:"The core components of a reinforcement learning system include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Agent"}),": The learning entity (robot or physical system)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Environment"}),": The physical world the agent interacts with"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"State"}),": The current situation of the agent in the environment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action"}),": What the agent can do in the environment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reward"}),": Feedback signal for the agent's actions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Policy"}),": Strategy that maps states to actions"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"q-learning-and-deep-q-networks",children:"Q-Learning and Deep Q-Networks"}),"\n",(0,a.jsx)(n.p,{children:"Q-Learning is a fundamental RL algorithm that learns the value of state-action pairs:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Q-Value"}),": Expected cumulative reward for taking an action in a state"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Bellman Equation"}),": Foundation for updating Q-values"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Deep Q-Networks (DQN)"}),": Using neural networks to approximate Q-values"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"policy-gradient-methods",children:"Policy Gradient Methods"}),"\n",(0,a.jsx)(n.p,{children:"Policy gradient methods directly optimize the policy function:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"REINFORCE"}),": Basic policy gradient algorithm"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Actor-Critic"}),": Combines value estimation with policy learning"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Proximal Policy Optimization (PPO)"}),": Advanced method with stable updates"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"system",children:"System"}),"\n",(0,a.jsx)(n.p,{children:"Implementing reinforcement learning for Physical AI often involves simulation environments like Gazebo, PyBullet, or MuJoCo, followed by transfer to real robots."}),"\n",(0,a.jsx)(n.h3,{id:"simple-q-learning-example",children:"Simple Q-Learning Example"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import numpy as np\n\nclass QLearningAgent:\n    def __init__(self, n_states, n_actions, learning_rate=0.1, discount=0.95, epsilon=0.1):\n        self.n_states = n_states\n        self.n_actions = n_actions\n        self.learning_rate = learning_rate\n        self.discount = discount\n        self.epsilon = epsilon\n        self.q_table = np.zeros((n_states, n_actions))\n\n    def choose_action(self, state):\n        # Epsilon-greedy action selection\n        if np.random.uniform(0, 1) < self.epsilon:\n            return np.random.choice(self.n_actions)  # Explore\n        else:\n            return np.argmax(self.q_table[state])    # Exploit\n\n    def learn(self, state, action, reward, next_state):\n        # Update Q-value using Bellman equation\n        td_target = reward + self.discount * np.max(self.q_table[next_state])\n        td_error = td_target - self.q_table[state, action]\n        self.q_table[state, action] += self.learning_rate * td_error\n\n# Example usage for a simple grid navigation task\nagent = QLearningAgent(n_states=25, n_actions=4)  # 5x5 grid, 4 directions\n"})}),"\n",(0,a.jsx)(n.h2,{id:"practice",children:"Practice"}),"\n",(0,a.jsx)(n.p,{children:"Now let's practice with some exercises to reinforce the concepts learned."}),"\n",(0,a.jsx)(n.h3,{id:"exercise-1-grid-world-navigation",children:"Exercise 1: Grid World Navigation"}),"\n",(0,a.jsx)(n.p,{children:"Implement a Q-learning agent to navigate a simple grid world environment."}),"\n",(0,a.jsx)(n.h3,{id:"exercise-2-robot-arm-control",children:"Exercise 2: Robot Arm Control"}),"\n",(0,a.jsx)(n.p,{children:"Use policy gradient methods to train a robot arm to reach target positions."}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"In this chapter, we've explored reinforcement learning concepts and their applications in Physical AI. We've covered Q-learning, policy gradients, and deep RL methods for learning complex behaviors through environmental interaction. We've also practiced with exercises to reinforce these concepts."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>s,x:()=>o});var t=i(6540);const a={},r=t.createContext(a);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);