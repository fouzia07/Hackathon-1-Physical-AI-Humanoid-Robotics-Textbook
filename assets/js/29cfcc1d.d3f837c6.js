"use strict";(globalThis.webpackChunkai_textbook=globalThis.webpackChunkai_textbook||[]).push([[9606],{7350(a){a.exports=JSON.parse('{"tag":{"label":"Vision-Language-Action","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/tags/vision-language-action","allTagsPath":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/tags","count":3,"items":[{"id":"modules/module-4-vla/chapter-1","title":"Chapter 1 - Vision-Language-Action Fundamentals","description":"Understanding the fundamentals of Vision-Language-Action integration for LLM-robot systems","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/modules/module-4-vla/chapter-1"},{"id":"modules/module-4-vla/chapter-1-vla-paradigm","title":"Chapter 1 - Vision-Language-Action Paradigm: Bridging Perception, Language, and Action","description":"Understanding the VLA concept and the role of LLMs in robotics for perception, planning, and action","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/modules/module-4-vla/chapter-1-vla-paradigm"},{"id":"modules/module-4-vla/chapter-1-vla-pipelines","title":"Chapter 1 - Vision-Language-Action Pipelines","description":"Understanding high-level VLA pipeline architectures for physical AI systems","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/modules/module-4-vla/chapter-1-vla-pipelines"}],"unlisted":false}}')}}]);