"use strict";(globalThis.webpackChunkai_textbook=globalThis.webpackChunkai_textbook||[]).push([[4250],{8453(e,n,i){i.d(n,{R:()=>o,x:()=>r});var t=i(6540);const a={},s=t.createContext(a);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(s.Provider,{value:n},e.children)}},9896(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"modules/module-4-vla/chapter-1-vla-pipelines","title":"Chapter 1 - Vision-Language-Action Pipelines","description":"Understanding high-level VLA pipeline architectures for physical AI systems","source":"@site/docs/modules/module-4-vla/chapter-1-vla-pipelines.md","sourceDirName":"modules/module-4-vla","slug":"/modules/module-4-vla/chapter-1-vla-pipelines","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/modules/module-4-vla/chapter-1-vla-pipelines","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-4-vla/chapter-1-vla-pipelines.md","tags":[{"inline":true,"label":"VLA","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/tags/vla"},{"inline":true,"label":"Vision-Language-Action","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/tags/vision-language-action"},{"inline":true,"label":"Pipeline Architecture","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/tags/pipeline-architecture"},{"inline":true,"label":"Physical AI","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/tags/physical-ai"},{"inline":true,"label":"textbook","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/tags/textbook"},{"inline":true,"label":"education","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/tags/education"}],"version":"current","frontMatter":{"title":"Chapter 1 - Vision-Language-Action Pipelines","description":"Understanding high-level VLA pipeline architectures for physical AI systems","tags":["VLA","Vision-Language-Action","Pipeline Architecture","Physical AI","textbook","education"],"learning_objectives":["Analyze the architecture of Vision-Language-Action pipelines","Understand the integration between perception, language, and action systems","Evaluate the role of LLMs in VLA coordination"],"summary":"This chapter examines Vision-Language-Action pipeline architectures, focusing on how visual perception, language understanding, and robotic action are coordinated in high-level physical AI systems."},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3 - Nav2 for Humanoid Navigation","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/modules/module-3-nvidia-isaac/chapter-3-nav2-humanoid-navigation"},"next":{"title":"Chapter 2 - Voice-to-Action Systems","permalink":"/Hackathon-1-Physical-AI-Humanoid-Robotics-Textbook/docs/modules/module-4-vla/chapter-2-voice-to-action-systems"}}');var a=i(4848),s=i(8453);const o={title:"Chapter 1 - Vision-Language-Action Pipelines",description:"Understanding high-level VLA pipeline architectures for physical AI systems",tags:["VLA","Vision-Language-Action","Pipeline Architecture","Physical AI","textbook","education"],learning_objectives:["Analyze the architecture of Vision-Language-Action pipelines","Understand the integration between perception, language, and action systems","Evaluate the role of LLMs in VLA coordination"],summary:"This chapter examines Vision-Language-Action pipeline architectures, focusing on how visual perception, language understanding, and robotic action are coordinated in high-level physical AI systems."},r="Chapter 1 - Vision-Language-Action Pipelines",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Concept",id:"concept",level:2},{value:"VLA Pipeline Architecture",id:"vla-pipeline-architecture",level:3},{value:"Coordination Mechanisms",id:"coordination-mechanisms",level:3},{value:"LLM Integration",id:"llm-integration",level:3},{value:"System",id:"system",level:2},{value:"High-Level VLA Architecture",id:"high-level-vla-architecture",level:3},{value:"Pipeline Coordination Strategies",id:"pipeline-coordination-strategies",level:3},{value:"Integration Interfaces",id:"integration-interfaces",level:3},{value:"Practice",id:"practice",level:2},{value:"Exercise 1: VLA Pipeline Analysis",id:"exercise-1-vla-pipeline-analysis",level:3},{value:"Exercise 2: Coordination Strategy Design",id:"exercise-2-coordination-strategy-design",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-1---vision-language-action-pipelines",children:"Chapter 1 - Vision-Language-Action Pipelines"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Analyze the architecture of Vision-Language-Action pipelines"}),"\n",(0,a.jsx)(n.li,{children:"Understand the integration between perception, language, and action systems"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate the role of LLMs in VLA coordination"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"concept",children:"Concept"}),"\n",(0,a.jsx)(n.p,{children:"Vision-Language-Action (VLA) pipelines represent a unified approach to embodied AI, where visual perception, natural language understanding, and robotic action are tightly integrated to create intelligent physical systems. These pipelines enable robots to understand natural language commands and execute complex tasks based on visual perception of their environment."}),"\n",(0,a.jsx)(n.h3,{id:"vla-pipeline-architecture",children:"VLA Pipeline Architecture"}),"\n",(0,a.jsx)(n.p,{children:"The fundamental VLA pipeline consists of three interconnected components:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Visual Perception Module"}),": Processes camera, LIDAR, and other sensory inputs to understand the environment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Language Understanding Module"}),": Interprets natural language commands and contextualizes them with visual information"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action Execution Module"}),": Translates high-level goals into low-level robotic behaviors"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"coordination-mechanisms",children:"Coordination Mechanisms"}),"\n",(0,a.jsx)(n.p,{children:"VLA systems employ various coordination mechanisms to ensure seamless interaction between components:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multimodal Fusion"}),": Combining visual and linguistic information for enhanced understanding"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Feedback Loops"}),": Using action outcomes to refine perception and language interpretation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Hierarchical Control"}),": Coordinating high-level planning with low-level execution"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"llm-integration",children:"LLM Integration"}),"\n",(0,a.jsx)(n.p,{children:"Large Language Models serve as the cognitive hub in VLA systems, providing:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Semantic Understanding"}),": Interpreting the meaning behind natural language commands"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reasoning Capabilities"}),": Planning complex sequences of actions based on goals"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Context Management"}),": Maintaining task context across multiple interactions"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"system",children:"System"}),"\n",(0,a.jsx)(n.p,{children:"The VLA system operates as an integrated cognitive architecture that processes multimodal inputs and generates coordinated robotic behaviors."}),"\n",(0,a.jsx)(n.h3,{id:"high-level-vla-architecture",children:"High-Level VLA Architecture"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Language Input Layer                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 Natural Language Command:                                         \u2502   \u2502\n\u2502  \u2502 "Pick up the red cup from the table"                              \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      LLM Processing Layer                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 Large Language Model:                                             \u2502   \u2502\n\u2502  \u2502 \u2022 Command Interpretation                                          \u2502   \u2502\n\u2502  \u2502 \u2022 Task Decomposition                                              \u2502   \u2502\n\u2502  \u2502 \u2022 Object Identification                                           \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Visual Perception Layer                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n\u2502  \u2502 Object      \u2502  \u2502 Scene       \u2502  \u2502 Spatial          \u2502                   \u2502\n\u2502  \u2502 Detection   \u2502  \u2502 Understanding\u2502  \u2502 Reasoning        \u2502                   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Action Planning Layer                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 Navigation      \u2502  \u2502 Manipulation    \u2502  \u2502 Execution                \u2502   \u2502\n\u2502  \u2502 Planning        \u2502  \u2502 Planning        \u2502  \u2502 Coordination             \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,a.jsx)(n.h3,{id:"pipeline-coordination-strategies",children:"Pipeline Coordination Strategies"}),"\n",(0,a.jsx)(n.p,{children:"The system employs several coordination strategies:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sequential Processing"}),": Processing inputs in a defined order from perception to action"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Parallel Processing"}),": Handling multiple modalities simultaneously with coordination points"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Iterative Refinement"}),": Continuously updating understanding based on new inputs and feedback"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"integration-interfaces",children:"Integration Interfaces"}),"\n",(0,a.jsx)(n.p,{children:"Key interfaces enable effective VLA integration:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multimodal Embeddings"}),": Common representation space for visual and linguistic information"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action Spaces"}),": Standardized interfaces between planning and execution modules"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Feedback Channels"}),": Communication pathways for outcome reporting and system adjustment"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"practice",children:"Practice"}),"\n",(0,a.jsx)(n.h3,{id:"exercise-1-vla-pipeline-analysis",children:"Exercise 1: VLA Pipeline Analysis"}),"\n",(0,a.jsx)(n.p,{children:"Analyze a given scenario and identify the flow of information through the VLA pipeline from language input to action execution."}),"\n",(0,a.jsx)(n.h3,{id:"exercise-2-coordination-strategy-design",children:"Exercise 2: Coordination Strategy Design"}),"\n",(0,a.jsx)(n.p,{children:"Design a coordination strategy for a VLA system that needs to handle ambiguous language commands with uncertain visual inputs."}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"This chapter explored Vision-Language-Action pipeline architectures, focusing on how visual perception, language understanding, and robotic action are coordinated in high-level physical AI systems. The integration of these components through LLM coordination enables sophisticated embodied AI capabilities while maintaining clear system boundaries and interfaces."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);